\documentclass[a4paper, 12pt]{article}

\usepackage{geometry}
\geometry{a4paper, margin=1in}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}

\usepackage{lipsum}
\usepackage[svgnames]{xcolor}

\usepackage{svg}

\usepackage{subcaption}


\title{Generating open source chess puzzles - notes}
\author{Aatu Selkee}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Tokenization}

Tokenization v1

\begin{itemize}
    \item Board\begin{itemize}
        \item PNBRQKpnbrqk. = 13 tokens
    \end{itemize}
    \item Side to move\begin{itemize}
        \item wb = 1 tokens (b already counted)
    \end{itemize}
    \item Castling\begin{itemize}
        \item KQkq. = 0 tokens (already counted)
    \end{itemize}
    \item En passant\begin{itemize}
        \item abcdefgh = 7 tokens (b already counted)
        \item 12345678 = 8 tokens
        \item -. = 1 tokens (. already counted)
        \item = 16 tokens
    \end{itemize}
    \item Half move counter\begin{itemize}
        \item 0123456789 = 2 (0 and 9 new tokens)
    \end{itemize}
    \item Full move counter\begin{itemize}
        \item 0123456789 = 0 (already counted)
    \end{itemize}
\end{itemize}

= 32 tokens

Total tokens do not match the number of tokens in the paper 31 (the most obvious is that "-" might be replaced with a "."). The length of the produced string is also 76 instead of 77 for some reason. This tokenization feels bad, as e.g. side to move b is completely different to board b (black to move vs black bishop).

Tokenization v2 (my current choice): Length 76, number of tokens 48 (own tokens e.g. for black bishop and black to move)


\section{Model architecture}

What should be used, pre- or post-normalization (\cite{feng2025generatingcreativechesspuzzles} says post (it says that the llama papers use post, but I think they use pre), but \cite{touvron2023llamaopenefficientfoundation} and \cite{touvron2023llama2openfoundation} use pre-normalization to improve stability.)

\textcolor{red}{If we want some new experiments, we can find out what masking schedule produces the best results after supervised learning and then apply RL to that model.}

\section{Metrics}

I would love to access the code they used to compute the metrics to see what the differences are.

Computing if the fen position is legal and unique is surprisingly costly (due to repeated Stockfish searches, which can be controlled by setting limits to the depth, nodes and time)

Overlap zero over the 1000 generated positions with the training set (probably due to generating the positions with only one active theme, which basically never happens in the training set).

\subsection{Uniqueness}

We remove one-movers, I think they might not? Similarly to the paper, we do not check if the best move wins enough (or draws when down in material) as Lichess puzzler does (\textcolor{red}{actually, we do check this!}). They mention minor differences in the implementation between Lichess puzzler and their metric computation. 

\subsection{Counter intuitiveness}

I think, in the end they compute $0.8\cdot depth_{move_{depth} == move_{high depth}} + 0.1\cdot value_{captured} \ge \tau_{cnt}$.

Perhaps new thing we could do is that we could modify the counter intuitiveness threshold $\tau_{cnt}$ by considering the difficulty rating of the puzzle (more difficult puzzle is more counter-intuitive)



\section{RL}

\cite{feng2025generatingcreativechesspuzzles} did not use the masked diffusion for the RL, and it will probably be harder than with the autoregressive model.

Compute the log-probability of the models in the same way as with autoregressive models (sum the log probabilities of the chosen tokens). The model must be called $K$ times, where $K$ is the amount of tokens (the latter tokens depend on the previous tokens and teacher forcing is not possible I think?). Hence, the computational complexity is a lot higher with masked diffusion than with an autoregressive model. \textcolor{red}{I may be wrong based on algorithm 2 of \cite{ruoss2024amortizedplanninglargescaletransformers}, as we call the model as many times as we have discretized the range [0, 1].}

\textcolor{Green}{Computation of the log-probability is intractable for these diffusion models. Therefore, as it is needed in RL, we will use the ELBO as a replacement as was done in \cite{ou2025principledrldiffusionllms}. We should compute the ELBO in exactly the same way as in SFT (for a fen, sample $t$, compute $\alpha_t$ and mask with probability, then compute the ELBO as in the paper, finally apply RL with the log probabilities replaced with the negative ELBOs). This paper may also be useful \cite{rojas2025improvingreasoningdiffusionlanguage}.}


\section{23.1.}

\clearpage

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        & Lichess puzzles & Masked Diffusion & Paper Masked Diffusion \\
        \hline
        Legal & 100\% & 96.8\% & 99.72\%\\
        \hline
        Unique & 81.7\% (95.25\%) & 9.67\% & 30.89\%\\
        \hline
        Counter-intuitive & 4.3\% (2.25\%) & 23.7\% & 1.11\%\\
        \hline
        Puzzle & 4.1\% (2.14\%) & 0.1\% & 0.34\%\\
        \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item Previously generated puzzles might not be amazing, as they were generated with a single uniformly random theme. The ones in the dataset have 3 or more themes almost every time. Most positions with a unique solution are mate-in-one.
    \item Uniqueness computation working pretty well (is probably slightly stricter than in the paper, as we remove one-movers (apart from mate-in-one), although we allow mates-in-one at the end of a puzzle to have multiple solutions)
    \item Uniqueness computation takes a long time for good puzzles that do not end in a checkmate, as we need to compute the entire solution line for the theme check. (computing the metrics (uniqueness and counter intuitiveness) for 1000 positions from the Lichess puzzle dataset took over 30 minutes, over 1.8 seconds per puzzle)
    \item FEN $\rightarrow$ themes working well. It was surprisingly well hidden in the Lichess Puzzler repository, but I implemented it to our code.
    \item Counter-intuitiveness behaves weird, as 1000 Lichess puzzles from the dataset have a realistic value, but our generations should probably not have a counter-intuitiveness of 23.7\%\dots
    \item It would be amazing, if we could get the implementations of the uniqueness and the counter-intuitiveness metrics 
    \item Talk with Arno Solin.
\end{itemize}

\section{26.1}

If the solution to a puzzle is unique, counter-intuitivity metric makes sense for the puzzles we have generated. If the solution is not unique, the engine has potential to find two good moves early and switch the correct move making the puzzle counter-intuitive.

Realized that the correct metrics should look the following (counter-intuitiveness was computed from all positions and not the ones with a unique solution):

\begin{table}[h]
    \centering
    \caption{The proportion of positions satisfying a criterion. An average is taken over 1000 generated positions from the model or 1000 randomly sampled positions from the Lichess Puzzle dataset. The values in the parenthesis are the corresponding values in \cite{feng2025generatingcreativechesspuzzles}.}
    \begin{tabular}{|c|c|c|}
        \hline
        & Lichess puzzles & Masked Diffusion \\
        \hline
        Legal & 100\% & 96.8\% (99.72\%)\\
        \hline
        Unique & 81.4\% (95.25\%) & 9.71\% (30.89\%)\\
        \hline
        Counter-intuitive & 5.0\% (2.25\%) & 1.1\% (1.11\%)\\
        \hline
        Puzzle & 4.1\% (2.14\%) & 0.1\% (0.34\%)\\
        \hline
    \end{tabular}
\end{table}

Could experiment with guidance mechanisms as in \cite{schiff2025simpleguidancemechanismsdiscrete}.

We could use a Gaussian quadrature ELBO estimate that has smaller variance from \cite{rojas2025improvingreasoningdiffusionlanguage}.


\section{27.1}
What group size should I use in GRPO? Currently, I set group size to batch size and use just one group.


\section{28.1}
ESPO now overfits to one position (if the reward is the negative number of nonzero tokens in the generated tokens, the model ends up generating just zeros)

\begin{figure}[h]
     \centering
     \begin{subfigure}[t]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/RL_overfit_high_variance.png}
         \caption{High Variance Estimate}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/RL_overfit_low_variance.png}
         \caption{Low Variance Estimate}
     \end{subfigure}
\end{figure}


Currently, I use the Gauss-Legendre quadrature to estimate the integral. I also use antithetic sampling (share masks with current model and reference model) and coupled sampling (use a mask and the inverse mask and average the losses) to reduce variance.

\section{29.1}

The RL is quite sensitive to the group size $G$ even for the simple reward.

$\mathcal{L}_{old}$ might be different to $\mathcal{L}_{ref}$ (currently I use them as the same). I probably need to change the training loop to store the elbo of the model when the positions are generated.

\section{2.2}

The low variance elbo estimate is currently working well. Previously, I had a sign error, as the model.elbo\_loss returned an upper bound for the negative log likelihood, which we minimized during supervised training. Now I take that into account in the ESPO loss computation.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/average_elbo_loss_over_time.png}
    \caption{y-axis negative ELBO integrand, x-axis t, negative ELBO is the area under this curve divided by a constant (20, the number of grid points). (Severi told me that this should have a peak in the middle and be low on the edges.)}
\end{figure}

\begin{figure}[ht]
     \centering
     \begin{subfigure}[t]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/average_losses.png}
         \caption{Cumulative average of the negative ELBO estimates.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/average_losses_hist.png}
         \caption{Histograms of the negative ELBO estimates.}
     \end{subfigure}
\end{figure}


\section{4.2}

If n\_gradient\_updates\_per\_generation is set to 1, the performance is great. If n\_gradient \_updates\_per\_generation is set higher and importance sampling is hence used, the performance is effected greatly by the $\epsilon$ parameter (needs to be larger to not clip as many gradients, e.g. $\epsilon = 0.5$ or even higher in the test case). If epsilon is low (e.g. $\epsilon = 0.1$), the model will not converge to a good solution. There is hence no bug in the code I think, although, the RL seems to converge a lot slower if n\_gradient\_updates\_per\_generation is eight compared to when it is one (way more gradient updates AND more new generations as well).


\begin{figure}[ht]
     \centering
     \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Rewards_for_betas.png}
        \caption{Rewards over time for different betas. If beta is zero, the model overfits completely. For larger betas, the rewards stay lower. No importance sampling is used (only one gradient update is made for each generation).}
    \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.48\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/Rewards_for_importance_sampling.png}
        \caption{Rewards over time if eight gradient updates are done for new generations. The convergence is way slower than in the image on the left (even $\frac{184}{8} = 23$ is slower than around $12$ for the image on the left).}
     \end{subfigure}
\end{figure}

\section{5.2}

In the reward function, the authors check if the entropy of the generated board is high enough, which I believe requires the autoregressive decomposition to be computed efficiently (some approximation is needed with the diffusion). Hence, currently, we do not employ the entropy condition for the reward and trust that the other diversity measures are enough.

\section{10.2}

The counter-intuitiveness metric is weird. My current understanding is that the authors of \cite{feng2025generatingcreativechesspuzzles} ended up finding only two nonzero features that effect the counter-intuitiveness (others had an effect as well, but the best model had only two features). 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.53\textwidth]{figures/counter_intuitiveness.png}
    \caption{Green parts are counter-intuitive, and the blue parts are not. Only captured material and critical point depth have nonzero weights, of which the coefficient of the critical depth is so much larger that the captured material has no effect.}
\end{figure}

\section{11.2}

Realized that the large clips I had previously were due to too large of a learning rate, which was not a problem as the reward function was very simple. With a smaller learning rate, the 8 gradient updates works and converges a lot better.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gradient_updates_1.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gradient_updates_8.png}
    \end{subfigure}
    \caption{Left with 1 gradient update (new generations every step) and right 8 gradient updates (new generation every eighth step). The learning rate in both pictures is $10^{-4}$. Now the larger amount of gradient updates works a lot faster (around 15 min vs around 30 min).}
\end{figure}

\section{12.2}

Because we generate $G$ positions with the same conditioning, the model often gets zero rewards and doesn't learn during RL. When removing the fen and pv distance checks, the model starts to change (initial test made the reward worse, when previously, it didn't move at all, meaning that there is a bug somewhere).

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/Nonzero_reward_positions.png}
    \caption{The rewards mostly make sense (0.5 might be too high reward for positions that have normal amount of pieces)}
\end{figure}

\section{18.2}

Found a good value for RL $\beta = 10^{-3}$. Implemented a rating model that predicts the difficulty of a puzzle based on themes and a fen. The following is an image of a checkpoint (training still continues and the model is getting better) of the errors of the model. The average error in the elo rating is around 278 and the median error is 223.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includesvg[width=\textwidth]{figures/rating_model_errors18000steps.svg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includesvg[width=\textwidth]{figures/rating_model_errors.svg}
    \end{subfigure}
    \caption{Errors on the test set after 18 000 training steps for the reward model (left) and after 10 500 steps (right).}
\end{figure}

\section{19.2}

Realized that \cite{feng2025generatingcreativechesspuzzles} only checks the pv distance for the first move and not the whole solution line during RL.

\begin{figure}[h]
    \includesvg[width=\textwidth]{figures/rating_model_errors20000.svg}
    \caption{Errors up to 20 000 steps. In the orange histogram, the themes used to help the regression are shuffled to make the model's dependence on the themes clearer.}
\end{figure}

\clearpage
\bibliography{references.bib}
\bibliographystyle{ieeetr}

\end{document}
