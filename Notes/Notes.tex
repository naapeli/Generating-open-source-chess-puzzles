\documentclass[a4paper, 12pt]{article}

\usepackage{geometry}
\geometry{a4paper, margin=1in}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}

\usepackage{lipsum}
\usepackage[svgnames]{xcolor}

\usepackage{subcaption}


\title{Generating open source chess puzzles - notes}
\author{Aatu Selkee}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Tokenization}

Tokenization v1

\begin{itemize}
    \item Board\begin{itemize}
        \item PNBRQKpnbrqk. = 13 tokens
    \end{itemize}
    \item Side to move\begin{itemize}
        \item wb = 1 tokens (b already counted)
    \end{itemize}
    \item Castling\begin{itemize}
        \item KQkq. = 0 tokens (already counted)
    \end{itemize}
    \item En passant\begin{itemize}
        \item abcdefgh = 7 tokens (b already counted)
        \item 12345678 = 8 tokens
        \item -. = 1 tokens (. already counted)
        \item = 16 tokens
    \end{itemize}
    \item Half move counter\begin{itemize}
        \item 0123456789 = 2 (0 and 9 new tokens)
    \end{itemize}
    \item Full move counter\begin{itemize}
        \item 0123456789 = 0 (already counted)
    \end{itemize}
\end{itemize}

= 32 tokens

Total tokens do not match the number of tokens in the paper 31 (the most obvious is that "-" might be replaced with a "."). The length of the produced string is also 76 instead of 77 for some reason. This tokenization feels bad, as e.g. side to move b is completely different to board b (black to move vs black bishop).

Tokenization v2 (my current choice): Length 76, number of tokens 48 (own tokens e.g. for black bishop and black to move)


\section{Model architecture}

What should be used, pre- or post-normalization (\cite{feng2025generatingcreativechesspuzzles} says post (it says that the llama papers use post, but I think they use pre), but \cite{touvron2023llamaopenefficientfoundation} and \cite{touvron2023llama2openfoundation} use pre-normalization to improve stability.)

\textcolor{red}{If we want some new experiments, we can find out what masking schedule produces the best results after supervised learning and then apply RL to that model.}

\section{Metrics}

I would love to access the code they used to compute the metrics to see what the differences are.

Computing if the fen position is legal and unique is surprisingly costly (due to repeated Stockfish searches, which can be controlled by setting limits to the depth, nodes and time)

Overlap zero over the 1000 generated positions with the training set (probably due to generating the positions with only one active theme, which basically never happens in the training set).

\subsection{Uniqueness}

We remove one-movers, I think they might not? Similarly to the paper, we do not check if the best move wins enough (or draws when down in material) as Lichess puzzler does (\textcolor{red}{actually, we do check this!}). They mention minor differences in the implementation between Lichess puzzler and their metric computation. 

\subsection{Counter intuitiveness}

I think, in the end they compute $0.8\cdot depth_{move_{depth} == move_{high depth}} + 0.1\cdot value_{captured} \ge \tau_{cnt}$.

Perhaps new thing we could do is that we could modify the counter intuitiveness threshold $\tau_{cnt}$ by considering the difficulty rating of the puzzle (more difficult puzzle is more counter-intuitive)



\section{RL}

\cite{feng2025generatingcreativechesspuzzles} did not use the masked diffusion for the RL, and it will probably be harder than with the autoregressive model.

Compute the log-probability of the models in the same way as with autoregressive models (sum the log probabilities of the chosen tokens). The model must be called $K$ times, where $K$ is the amount of tokens (the latter tokens depend on the previous tokens and teacher forcing is not possible I think?). Hence, the computational complexity is a lot higher with masked diffusion than with an autoregressive model. \textcolor{red}{I may be wrong based on algorithm 2 of \cite{ruoss2024amortizedplanninglargescaletransformers}, as we call the model as many times as we have discretized the range [0, 1].}

\textcolor{Green}{Computation of the log-probability is intractable for these diffusion models. Therefore, as it is needed in RL, we will use the ELBO as a replacement as was done in \cite{ou2025principledrldiffusionllms}. We should compute the ELBO in exactly the same way as in SFT (for a fen, sample $t$, compute $\alpha_t$ and mask with probability, then compute the ELBO as in the paper, finally apply RL with the log probabilities replaced with the negative ELBOs). This paper may also be useful \cite{rojas2025improvingreasoningdiffusionlanguage}.}


\section{23.1.}

\clearpage

\begin{table}
    \begin{tabular}{|c|c|c|c|}
        \hline
        & Lichess puzzles & Masked Diffusion & Paper Masked Diffusion \\
        \hline
        Legal & 100\% & 96.8\% & 99.72\%\\
        \hline
        Unique & 81.7\% (95.25\%) & 9.67\% & 30.89\%\\
        \hline
        Counter-intuitive & 4.3\% (2.25\%) & 23.7\% & 1.11\%\\
        \hline
        Puzzle & 4.1\% (2.14\%) & 0.1\% & 0.34\%\\
        \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item Previously generated puzzles might not be amazing, as they were generated with a single uniformly random theme. The ones in the dataset have 3 or more themes almost every time. Most positions with a unique solution are mate-in-one.
    \item Uniqueness computation working pretty well (is probably slightly stricter than in the paper, as we remove one-movers (apart from mate-in-one), although we allow mates-in-one at the end of a puzzle to have multiple solutions)
    \item Uniqueness computation takes a long time for good puzzles that do not end in a checkmate, as we need to compute the entire solution line for the theme check. (computing the metrics (uniqueness and counter intuitiveness) for 1000 positions from the Lichess puzzle dataset took over 30 minutes, over 1.8 seconds per puzzle)
    \item FEN $\rightarrow$ themes working well. It was surprisingly well hidden in the Lichess Puzzler repository, but I implemented it to our code.
    \item Counter-intuitiveness behaves weird, as 1000 Lichess puzzles from the dataset have a realistic value, but our generations should probably not have a counter-intuitiveness of 23.7\%\dots
    \item It would be amazing, if we could get the implementations of the uniqueness and the counter-intuitiveness metrics 
    \item Talk with Arno Solin.
\end{itemize}

\section{26.1}

If the solution to a puzzle is unique, counter-intuitivity metric makes sense for the puzzles we have generated. If the solution is not unique, the engine has potential to find two good moves early and switch the correct move making the puzzle counter-intuitive.

Realized that the correct metrics should look the following (counter-intuitiveness was computed from all positions and not the ones with a unique solution):

\begin{table}[h]
    \centering
    \caption{The proportion of positions satisfying a criterion. An average is taken over 1000 generated positions from the model or 1000 randomly sampled positions from the Lichess Puzzle dataset. The values in the parenthesis are the corresponding values in \cite{feng2025generatingcreativechesspuzzles}.}
    \begin{tabular}{|c|c|c|}
        \hline
        & Lichess puzzles & Masked Diffusion \\
        \hline
        Legal & 100\% & 96.8\% (99.72\%)\\
        \hline
        Unique & 81.4\% (95.25\%) & 9.71\% (30.89\%)\\
        \hline
        Counter-intuitive & 5.0\% (2.25\%) & 1.1\% (1.11\%)\\
        \hline
        Puzzle & 4.1\% (2.14\%) & 0.1\% (0.34\%)\\
        \hline
    \end{tabular}
\end{table}

Could experiment with guidance mechanisms as in \cite{schiff2025simpleguidancemechanismsdiscrete}.

We could use a Gaussian quadrature ELBO estimate that has smaller variance from \cite{rojas2025improvingreasoningdiffusionlanguage}.


\section{27.1}
What group size should I use in GRPO? Currently, I set group size to batch size and use just one group.


\section{28.1}
ESPO now overfits to one position (if the reward is the negative number of nonzero tokens in the generated tokens, the model ends up generating just zeros)

\begin{figure}[h]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/RL_overfit_high_variance.png}
         \caption{High Variance Estimate}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/RL_overfit_low_variance.png}
         \caption{Low Variance Estimate}
     \end{subfigure}
\end{figure}

Currently, I use the Gauss-Legendre quadrature to estimate the integral. I also use antithetic sampling (share masks with current model and reference model) and coupled sampling (use a mask and the inverse mask and average the losses) to reduce variance.

\section{29.1}

The RL is quite sensitive to the group size $G$ even for the simple reward.

$\mathcal{L}_{old}$ might be different to $\mathcal{L}_{ref}$ (currently I use them as the same). I probably need to change the training loop to store the elbo of the model when the positions are generated.

\section{2.2}

The low variance elbo estimate is currently working well. Previously, I had a sign error, as the model.elbo\_loss returned an upper bound for the negative log likelihood, which we minimized during supervised training. Now I take that into account in the ESPO loss computation.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/average_elbo_loss_over_time.png}
    \caption{y-axis ELBO integrand, x-axis t, ELBO is the area under this curve divided by a constant (20, the number of grid points). (Severi told me that this should have a peak in the middle and be low on the edges.)}
\end{figure}

\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/average_losses.png}
         \caption{Cumulative average of the ELBO estimates.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/average_losses_hist.png}
         \caption{Histograms of the ELBO estimates.}
     \end{subfigure}
\end{figure}


\section{4.2}

Importance sampling has some bug (or performs unpredictably for some other reason), but if n\_gradient\_updates\_per\_generation is set to 1, the performance is great.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Rewards_for_betas.png}
    \caption{Rewards over time for different betas. If beta is zero, the model overfits completely. For larger betas, the rewards stay lower. No importance sampling is used (only one gradient update is made for each generation).}
\end{figure}



\clearpage
\bibliography{references.bib}
\bibliographystyle{ieeetr}

\end{document}
